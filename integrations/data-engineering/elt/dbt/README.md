## Integration: dbt â†’ QALITA (Beta)

<p align="center">
  <img width="800px" height="auto" src="../../../../img/integration/qalita-x-dbt.png"/>
</p>

**Goal:** export data quality metadata and test results from dbt runs into QALITA.

**High-level flow:**

1. dbt generates artifacts (`manifest.json`, `run_results.json`) and test results.
2. A post-run step invokes QALITA CLI (`tools-cli`) or direct REST calls to `app-backend` to push:

   * Source, dataset, and model metadata
   * Test definitions and outcomes
   * Run context (job id, environment, git sha)
3. QALITA ingests and persists via `app-backend` (`routers/v2`, `database/model`) and triggers scoring/jobs.

**Backend touchpoints:**

* `app-backend/src/backend/routers/v2/` for ingestion endpoints (e.g., sources, datasets, checks, runs, issues).
* `app-backend/src/backend/database/model/` for persisted entities (Job, Source, Check, Result).
* Optional background workers for async processing and scoring.

**Implementation options:**

* Use QALITA CLI (`/home/<user>/platform/tools-cli`) in a dbt post-hook or CI step:

  * Collect artifacts: `target/manifest.json`, `target/run_results.json`
  * Run:

    ```bash
    qalita ingest dbt --manifest path --results path --project <name> --env <name>
    ```
* Direct REST integration from CI using an API token.

**Docs references:**

* Platform quick start: [https://doc.qalita.io/docs/platform/quick-start](https://doc.qalita.io/docs/platform/quick-start)
* Packs and sources: [https://doc.qalita.io/docs/platform/user-guides/data-engineering/packs](https://doc.qalita.io/docs/platform/user-guides/data-engineering/packs)
* CLI quick start: [https://doc.qalita.io/docs/cli/quick-start](https://doc.qalita.io/docs/cli/quick-start)

**Next steps:**

* Map dbt test statuses to QALITA status schema
* Define idempotency keys per run (project, environment, git sha, run id)
* Provide example CI YAML for dbt Cloud and GitHub Actions

---

### Ready-to-use Example (Open Data + DuckDB)

This example shows how to use dbt with DuckDB from an Open Data seed (air quality), then ingest dbt artifacts into QALITA.

* **Folder**: `/home/<user>/platform/tutorials/integrations/data-engineering/elt/dbt/example_dbt`
* **Database**: DuckDB (local `opendata.duckdb` file)
* **Seed**: `seeds/opendata_airquality.csv`
* **Models**: `models/staging/stg_airquality.sql`, `models/marts/airquality_metrics.sql`

#### Prerequisites

* Python 3.10+
* `make`
* Optional: QALITA CLI (`qalita`) in PATH for ingestion

#### Quick start

1. Go to the example:

   ```bash
   cd /home/<user>/platform/tutorials/integrations/data-engineering/elt/dbt/example_dbt
   ```
2. Create the environment, install dbt-duckdb, load the seed, build, and test:

   ```bash
   make all
   ```
3. (Optional) Ingest dbt artifacts into QALITA:

   ```bash
   make ingest
   ```

**Notes:**

* The `Makefile` configures `DBT_PROFILES_DIR` to use the local `profiles.yml`.
* dbt artifacts (`target/manifest.json`, `target/run_results.json`) are generated by `make run/test`.
* Ingestion calls:

  ```bash
  qalita ingest dbt --manifest target/manifest.json --results target/run_results.json --project opendata_dbt --env dev
  ```

#### Key files

* `dbt_project.yml` and `profiles.yml`: project and DuckDB profile configuration
* `seeds/opendata_airquality.csv`: Open Data sample (air quality)
* `models/staging/stg_airquality.sql`: cleaning/typing of measurements
* `models/marts/airquality_metrics.sql`: aggregations (min/max/average, volume)
* `models/schema.yml`: basic tests (not\_null, accepted\_values)
* `scripts/ingest_qalita.sh`: ingestion into QALITA (requires CLI)

#### Customization

* Replace `seeds/opendata_airquality.csv` with your own data (same columns) and re-run `make seed && make run`.
* Adapt the aggregation logic in `models/marts/airquality_metrics.sql` to your KPIs.
* Adjust the ingestion project/env via variables in `scripts/ingest_qalita.sh`.
